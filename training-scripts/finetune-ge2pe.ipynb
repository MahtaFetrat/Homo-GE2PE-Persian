{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Env"
      ],
      "metadata": {
        "id": "o0kpeSgaBbgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode==1.4.0 evaluate==0.4.6 transformers==4.49.0 jiwer==4.0.0"
      ],
      "metadata": {
        "id": "gBz_s9gYBbMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from dataclasses import dataclass\n",
        "from typing import Union, Dict, List\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "import argparse\n",
        "import torch\n",
        "import evaluate\n",
        "\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Union, Dict, List, Optional\n",
        "from transformers import AdamW, AutoTokenizer, T5ForConditionalGeneration, T5Config\n",
        "from transformers import (\n",
        "    DataCollator,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "set_seed(41)"
      ],
      "metadata": {
        "id": "A8v5ZqyN_mt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Data"
      ],
      "metadata": {
        "id": "9KWh6BbYAeFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct_url = \"https://huggingface.co/datasets/MahtaFetrat/HomoRich-G2P-Persian/resolve/main/data/train-01.parquet\"\n",
        "df = pd.read_parquet(correct_url)\n",
        "\n",
        "df.to_csv('PersianG2P.csv')"
      ],
      "metadata": {
        "id": "e6ciXCJa83TD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/datasets/MahtaFetrat/HomoRich-G2P-Persian/resolve/main/augmented_data/PersianG2P_augmented.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W05q3DqgB6Ox",
        "outputId": "24542e2e-a067-44b7-a1df-03b4dd4f672b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-30 07:32:38--  https://huggingface.co/datasets/MahtaFetrat/HomoRich-G2P-Persian/resolve/main/augmented_data/PersianG2P_augmented.csv\n",
            "Resolving huggingface.co (huggingface.co)... 3.166.152.105, 3.166.152.110, 3.166.152.44, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.166.152.105|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/677246f593724e487d680fd1/4f2faec3066594a874559101086d38d02c12057e746a3cdc6c53fef7cc8bc3ac?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251030%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251030T073238Z&X-Amz-Expires=3600&X-Amz-Signature=072f4f4afa8fd8e45ccc53b4913f3ac2909e1e479e53ad4f220ec4235e5ed8e0&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27PersianG2P_augmented.csv%3B+filename%3D%22PersianG2P_augmented.csv%22%3B&response-content-type=text%2Fcsv&x-id=GetObject&Expires=1761813158&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTgxMzE1OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NzcyNDZmNTkzNzI0ZTQ4N2Q2ODBmZDEvNGYyZmFlYzMwNjY1OTRhODc0NTU5MTAxMDg2ZDM4ZDAyYzEyMDU3ZTc0NmEzY2RjNmM1M2ZlZjdjYzhiYzNhYyoifV19&Signature=MREWlQfben1z%7ErLSOd9pn%7EgPFP53xu9k9OKngMQygt9Sa7qkp4KvM%7EU8025WGW4eB-r%7EzZ-i%7Ewodawi8-4ZrUCNvB%7EcBviE5gH9Upv7WTV4TLRgiG5OuRV4tbaAJdn88mbBDCJ0kPperwRecQNGFzejucecy585e3ALZpc%7EtKnkCvm%7EVrVGmEgc-DX61AGcji7dpXWySRrvS1NrZMlRQG6dgOuAN7sN-7ofkdlfc3IivSNTBBbn5e3kfhrylfp5cPvyAmi%7E9nxZcSY%7ENHJgS3eox1V%7EWiF8B5JbYV50gYUfhMclHthTleg%7ERfm%7ERj49Rn%7EFWPcEghm0NTUGf5p-nPA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-10-30 07:32:38--  https://cas-bridge.xethub.hf.co/xet-bridge-us/677246f593724e487d680fd1/4f2faec3066594a874559101086d38d02c12057e746a3cdc6c53fef7cc8bc3ac?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251030%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251030T073238Z&X-Amz-Expires=3600&X-Amz-Signature=072f4f4afa8fd8e45ccc53b4913f3ac2909e1e479e53ad4f220ec4235e5ed8e0&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27PersianG2P_augmented.csv%3B+filename%3D%22PersianG2P_augmented.csv%22%3B&response-content-type=text%2Fcsv&x-id=GetObject&Expires=1761813158&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTgxMzE1OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NzcyNDZmNTkzNzI0ZTQ4N2Q2ODBmZDEvNGYyZmFlYzMwNjY1OTRhODc0NTU5MTAxMDg2ZDM4ZDAyYzEyMDU3ZTc0NmEzY2RjNmM1M2ZlZjdjYzhiYzNhYyoifV19&Signature=MREWlQfben1z%7ErLSOd9pn%7EgPFP53xu9k9OKngMQygt9Sa7qkp4KvM%7EU8025WGW4eB-r%7EzZ-i%7Ewodawi8-4ZrUCNvB%7EcBviE5gH9Upv7WTV4TLRgiG5OuRV4tbaAJdn88mbBDCJ0kPperwRecQNGFzejucecy585e3ALZpc%7EtKnkCvm%7EVrVGmEgc-DX61AGcji7dpXWySRrvS1NrZMlRQG6dgOuAN7sN-7ofkdlfc3IivSNTBBbn5e3kfhrylfp5cPvyAmi%7E9nxZcSY%7ENHJgS3eox1V%7EWiF8B5JbYV50gYUfhMclHthTleg%7ERfm%7ERj49Rn%7EFWPcEghm0NTUGf5p-nPA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.173.166.74, 18.173.166.82, 18.173.166.5, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.173.166.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67422075 (64M) [text/csv]\n",
            "Saving to: ‘PersianG2P_augmented.csv’\n",
            "\n",
            "PersianG2P_augmente 100%[===================>]  64.30M   167MB/s    in 0.4s    \n",
            "\n",
            "2025-10-30 07:32:39 (167 MB/s) - ‘PersianG2P_augmented.csv’ saved [67422075/67422075]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Base Checkpoint"
      ],
      "metadata": {
        "id": "6V2n1TDjAg5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown -q 1CrCX8SNhMcmi3KogffFaS4pSaC0t73nJ -O checkpoint-320.zip\n",
        "!unzip checkpoint-320.zip\n",
        "!mv content/checkpoint-320 checkpoint-320A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8xIVHmnA0UL",
        "outputId": "3bf675eb-c086-4ec8-a334-069480547e2a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  checkpoint-320.zip\n",
            "replace content/checkpoint-320/special_tokens_map.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: content/checkpoint-320/special_tokens_map.json  \n",
            "  inflating: content/checkpoint-320/scheduler.pt  \n",
            "  inflating: content/checkpoint-320/config.json  \n",
            "  inflating: content/checkpoint-320/rng_state.pth  \n",
            "  inflating: content/checkpoint-320/pytorch_model.bin  \n",
            "  inflating: content/checkpoint-320/tokenizer_config.json  \n",
            "  inflating: content/checkpoint-320/trainer_state.json  \n",
            "  inflating: content/checkpoint-320/training_args.bin  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning"
      ],
      "metadata": {
        "id": "IxVoGwjqAinl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "\n",
        "    batch['input_ids'] = batch['Grapheme']\n",
        "    batch['labels'] = batch['Mapped Phoneme']\n",
        "\n",
        "    return batch\n",
        "\n",
        "# %%\n",
        "# Data collator for padding\n",
        "@dataclass\n",
        "class DataCollatorWithPadding:\n",
        "    tokenizer: AutoTokenizer\n",
        "    padding: Union[bool, str] = True\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        words = [feature[\"input_ids\"] for feature in features]\n",
        "        prons = [feature[\"labels\"] for feature in features]\n",
        "        batch = self.tokenizer(words, padding=self.padding, add_special_tokens=False, return_attention_mask=True, return_tensors='pt')\n",
        "        pron_batch = self.tokenizer(prons, padding=self.padding, add_special_tokens=True, return_attention_mask=True, return_tensors='pt')\n",
        "        batch['labels'] = pron_batch['input_ids'].masked_fill(pron_batch.attention_mask.ne(1), -100)\n",
        "        return batch\n",
        "\n",
        "# %%\n",
        "# Compute metrics (CER and WER)\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"cer\": cer, 'wer': wer}\n",
        "\n",
        "# setting the evaluation metrics\n",
        "cer_metric = evaluate.load(\"cer\")\n",
        "wer_metric = evaluate.load('wer')"
      ],
      "metadata": {
        "id": "FrF-yR0o_7MJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1"
      ],
      "metadata": {
        "id": "kqeRm99jAI_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pronuncation_dictionary(path, train=True, homograph_only=False, human=False) -> Dataset:\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(path, index_col=[0])\n",
        "\n",
        "    if homograph_only:\n",
        "        if human:\n",
        "            df = df[df['Source'] == 'human']\n",
        "        if not human:\n",
        "            df = df[df['Source'] != 'human']\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    df = df.drop(['Source', 'Source ID'], axis=1)\n",
        "\n",
        "    # Drop rows where 'Phoneme' is NaN\n",
        "    df = df.dropna(subset=['Mapped Phoneme'])\n",
        "\n",
        "    # Filter rows based on phoneme length\n",
        "    Plen = np.array([len(i) for i in df['Mapped Phoneme']])\n",
        "    df = df.iloc[Plen < 512, :]\n",
        "\n",
        "    # Filter rows based on 'Homograph Grapheme' column\n",
        "    if homograph_only:\n",
        "        df = df[df['Homograph Grapheme'].notna() & (df['Homograph Grapheme'] != '')]\n",
        "    else:\n",
        "        df = df[df['Homograph Grapheme'].isna() | (df['Homograph Grapheme'] == '')]\n",
        "\n",
        "    # Shuffle the DataFrame\n",
        "    df = df.sample(frac=1)\n",
        "\n",
        "    # Split into train and test sets\n",
        "    if train:\n",
        "        return Dataset.from_pandas(df.iloc[:len(df)-90, :])\n",
        "    else:\n",
        "        return Dataset.from_pandas(df.iloc[len(df)-90:, :])\n",
        "\n",
        "# %%\n",
        "# Load datasets (only rows with 'Homograph Grapheme')\n",
        "train_data = load_pronuncation_dictionary('PersianG2P.csv', train=True)\n",
        "train_data = train_data.map(prepare_dataset)\n",
        "train_dataset = train_data\n",
        "\n",
        "dev_data = load_pronuncation_dictionary('PersianG2P.csv', train=False)\n",
        "dev_data = dev_data.map(prepare_dataset)\n",
        "dev_dataset = dev_data\n",
        "\n",
        "# Load tokenizer and model from checkpoint\n",
        "checkpoint_path = \"checkpoint-320\"  # Path to your checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(checkpoint_path)\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Training arguments (default values)\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./phase1-30-ep\",  # Directory to save the fine-tuned model\n",
        "    predict_with_generate=True,\n",
        "    generation_num_beams=5,\n",
        "    generation_max_length=512,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=32,  # Default batch size\n",
        "    per_device_eval_batch_size=100,  # Default batch size\n",
        "    num_train_epochs=5,  # Fewer epochs for this step\n",
        "    learning_rate=5e-4,  # Default learning rate\n",
        "    warmup_steps=1000,  # Default warmup steps\n",
        "    logging_steps=1000,  # Default logging steps\n",
        "    save_steps=4000,  # Default save steps\n",
        "    eval_steps=1000,  # Default evaluation steps\n",
        "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
        "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
        "    fp16=False,  # Disable FP16 by default\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./phase1-30-ep\")\n",
        "\n",
        "# %%\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract training and validation loss from the log history\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "for log in trainer.state.log_history:\n",
        "    if \"loss\" in log:\n",
        "        train_loss.append(log[\"loss\"])\n",
        "    if \"eval_loss\" in log:\n",
        "        val_loss.append(log[\"eval_loss\"])\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_loss, label=\"Training Loss\", marker=\"o\")\n",
        "plt.plot(val_loss, label=\"Validation Loss\", marker=\"o\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Save the plot to disk\n",
        "plt.savefig(\"phase1-30-ep.png\")\n",
        "\n",
        "# Optionally, close the plot to free up memory\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "_O1bSPnnAO3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2"
      ],
      "metadata": {
        "id": "zpxaQdpPAP-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Load datasets (only rows with 'Homograph Grapheme')\n",
        "train_data = load_pronuncation_dictionary('PersianG2P.csv',\n",
        "                                          train=True,\n",
        "                                          homograph_only=True)\n",
        "train_data = train_data.map(prepare_dataset)\n",
        "train_dataset = train_data\n",
        "\n",
        "dev_data = load_pronuncation_dictionary('PersianG2P.csv',\n",
        "                                        train=False,\n",
        "                                        homograph_only=True)\n",
        "dev_data = dev_data.map(prepare_dataset)\n",
        "dev_dataset = dev_data\n",
        "\n",
        "# Load tokenizer and model from the previous fine-tuning step\n",
        "checkpoint_path = \"./phase1-30-ep\"  # Path to the model from Step 1\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(checkpoint_path)\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Training arguments (default values)\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./phase2-30-ep\",  # Directory to save the final fine-tuned model\n",
        "    predict_with_generate=True,\n",
        "    generation_num_beams=5,\n",
        "    generation_max_length=512,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=32,  # Default batch size\n",
        "    per_device_eval_batch_size=100,  # Default batch size\n",
        "    num_train_epochs=30,  # More epochs for this step\n",
        "    learning_rate=5e-4,  # Lower learning rate for fine-tuning\n",
        "    warmup_steps=1000,  # Default warmup steps\n",
        "    logging_steps=1000,  # Default logging steps\n",
        "    save_steps=4000,  # Default save steps\n",
        "    eval_steps=1000,  # Default evaluation steps\n",
        "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
        "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
        "    fp16=False,  # Disable FP16 by default\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./phase2-30-ep\")\n",
        "\n",
        "\n",
        "# %%\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract training and validation loss from the log history\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "for log in trainer.state.log_history:\n",
        "    if \"loss\" in log:\n",
        "        train_loss.append(log[\"loss\"])\n",
        "    if \"eval_loss\" in log:\n",
        "        val_loss.append(log[\"eval_loss\"])\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_loss, label=\"Training Loss\", marker=\"o\")\n",
        "plt.plot(val_loss, label=\"Validation Loss\", marker=\"o\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Save the plot to disk\n",
        "plt.savefig(\"phase2-30-ep.png\")\n",
        "\n",
        "# Optionally, close the plot to free up memory\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "ITWLlr_7ATsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3"
      ],
      "metadata": {
        "id": "AmrZNGn-AU5W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjKO05jC7sb5"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# Load datasets (only rows with 'Homograph Grapheme')\n",
        "train_data = load_pronuncation_dictionary('PersianG2P_augmented.csv',\n",
        "                                          train=True,\n",
        "                                          homograph_only=True,\n",
        "                                          human=True)\n",
        "train_data = train_data.map(prepare_dataset)\n",
        "train_dataset = train_data\n",
        "\n",
        "dev_data = load_pronuncation_dictionary('PersianG2P_augmented.csv',\n",
        "                                        train=False,\n",
        "                                        homograph_only=True,\n",
        "                                        human=True)\n",
        "dev_data = dev_data.map(prepare_dataset)\n",
        "dev_dataset = dev_data\n",
        "\n",
        "# Load tokenizer and model from the previous fine-tuning step\n",
        "checkpoint_path = \"./phase2-30-ep\"  # Path to the model from Step 1\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(checkpoint_path)\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Training arguments (default values)\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./phase3-30-ep\",  # Directory to save the final fine-tuned model\n",
        "    predict_with_generate=True,\n",
        "    generation_num_beams=5,\n",
        "    generation_max_length=512,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=32,  # Default batch size\n",
        "    per_device_eval_batch_size=100,  # Default batch size\n",
        "    num_train_epochs=50,  # More epochs for this step\n",
        "    learning_rate=5e-4,  # Lower learning rate for fine-tuning\n",
        "    warmup_steps=1000,  # Default warmup steps\n",
        "    logging_steps=1000,  # Default logging steps\n",
        "    save_steps=4000,  # Default save steps\n",
        "    eval_steps=1000,  # Default evaluation steps\n",
        "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
        "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
        "    fp16=False,  # Disable FP16 by default\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./phase3-30-ep\")\n",
        "\n",
        "\n",
        "# %%\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract training and validation loss from the log history\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "for log in trainer.state.log_history:\n",
        "    if \"loss\" in log:\n",
        "        train_loss.append(log[\"loss\"])\n",
        "    if \"eval_loss\" in log:\n",
        "        val_loss.append(log[\"eval_loss\"])\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_loss, label=\"Training Loss\", marker=\"o\")\n",
        "plt.plot(val_loss, label=\"Validation Loss\", marker=\"o\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Save the plot to disk\n",
        "plt.savefig(\"phase3-30-ep.png\")\n",
        "\n",
        "# Optionally, close the plot to free up memory\n",
        "plt.close()\n",
        "\n"
      ]
    }
  ]
}